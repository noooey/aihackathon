{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Lzg9_pryJc7b",
        "outputId": "18a211a4-0e88-4585-a2e9-b4eb858c6f22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n입출력 양식을\\n준수해 주십시오\\n\\n###  INPUT ###\\nimport pandas as pd\\ninput_data = pd.read_csv(\\'SAMPLE_DATA.csv\\')\\n\\n{\\n\\n        Write codes...\\n\\n    Training model name : model\\n    ex) y_pred = model.predict(X_test)\\n\\n\\n}\\n\\n\\n### OUTPUT ###\\nprint(\"RMSE:\", rmse)\\nprint(\"R2_score:\", r2score)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#  -------------------------------------------\n",
        "\"\"\"\n",
        "입출력 양식을\n",
        "준수해 주십시오\n",
        "\n",
        "###  INPUT ###\n",
        "import pandas as pd\n",
        "input_data = pd.read_csv('SAMPLE_DATA.csv')\n",
        "\n",
        "{\n",
        "\n",
        "        Write codes...\n",
        "\n",
        "    Training model name : model\n",
        "    ex) y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "### OUTPUT ###\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2_score:\", r2score)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RDgwq6d2Jc7j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # 이 줄은 아래 HistGradientBoostingRegressor를 사용하기 전에 필요합니다.\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O6N8TpfJc7l",
        "outputId": "19018b4b-41ae-4924-bd02-ff3a6bdf9c97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84840, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "###  INPUT ###\n",
        "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
        "input_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "spuLsHiPJc7m"
      },
      "outputs": [],
      "source": [
        "df = input_data.drop(columns=['frmDist'])\n",
        "df = df.sort_values(by='date')\n",
        "\n",
        "# 데이터를 훈련 세트와 테스트 세트로 분할\n",
        "'''\n",
        "데이터를 훈련 세트와 테스트 세트로 나누는 데 사용됩니다.\n",
        "이는 모델의 성능을 평가하기 위해 데이터를 분리하는 일반적인 절차입니다.\n",
        "'''\n",
        "# 데이터셋을 data, target으로 변수분리\n",
        "X = df[df.drop(columns=['outtrn_cumsum','HeatingEnergyUsage_cumsum']).columns]\n",
        "Y = df[['outtrn_cumsum','HeatingEnergyUsage_cumsum']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X와 y를 numpy 배열로 변환\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values"
      ],
      "metadata": {
        "id": "Q0pK967nbttZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f68fT8LIJc7o"
      },
      "outputs": [],
      "source": [
        "class SimpleEnsemble(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = [MultiOutputRegressor(model) for model in models]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "#linearRegressor = LinearRegression()\n",
        "#lassocv = LassoCV()\n",
        "#ridge = Ridge()\n",
        "#lassolarscv = LassoLarsCV()\n",
        "#elasticnetcv = ElasticNetCV()\n",
        "model_xgb = xgb.XGBRegressor()\n",
        "GBoost = GradientBoostingRegressor()\n",
        "LightGB = lgb.LGBMRegressor()\n",
        "RF = RandomForestRegressor()\n",
        "ETR = ExtraTreesRegressor()\n",
        "DT = DecisionTreeRegressor()\n",
        "HGBR = HistGradientBoostingRegressor()\n",
        "#AdaBoost = AdaBoostRegressor()\n",
        "\n",
        "# 테스트 세트에 대한 예측\n",
        "models = [GBoost, LightGB, RF, model_xgb, ETR, DT, HGBR]\n",
        "# model = SimpleEnsemble(models)\n",
        "\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.base import clone\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "class StackingModel(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for model in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
        "\n",
        "        # 메타 모델의 훈련 데이터를 저장하기 위한 빈 배열\n",
        "        meta_train_data = np.zeros((X.shape[0], len(self.base_models) * y.shape[1]))\n",
        "\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                multi_target_instance = MultiOutputRegressor(instance)\n",
        "                self.base_models_[i].append(multi_target_instance)\n",
        "                multi_target_instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = multi_target_instance.predict(X[holdout_index])\n",
        "                meta_train_data[holdout_index, i*y.shape[1]:(i+1)*y.shape[1]] = y_pred\n",
        "\n",
        "        self.meta_model_ = MultiOutputRegressor(self.meta_model_)\n",
        "        self.meta_model_.fit(meta_train_data, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_models) * y_train.shape[1]))\n",
        "\n",
        "        for i, base_models in enumerate(self.base_models_):\n",
        "            for model in base_models:\n",
        "                y_pred = model.predict(X)\n",
        "                meta_features[:, i*y_train.shape[1]:(i+1)*y_train.shape[1]] += y_pred\n",
        "        meta_features /= self.n_folds\n",
        "\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "\n",
        "\n",
        "# 스태킹 모델 초기화\n",
        "stacked_model = StackingModel(base_models=models, meta_model=LinearRegression())\n",
        "\n",
        "# 모델 훈련 및 예측\n",
        "stacked_model.fit(X_train.values, y_train.values)\n",
        "y_pred = stacked_model.predict(X_test.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBoqdZzyckzm",
        "outputId": "ee60d8b4-fe04-4947-8677-b93ec3e88ffc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019783 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8960\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25387.094780\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020188 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8960\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 145332.145115\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020224 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8967\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25509.313933\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030068 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8967\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 143045.715727\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033152 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8959\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25546.404384\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019383 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8959\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 144614.905281\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019544 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8970\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25794.964967\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020249 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8970\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 142901.301509\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25451.837987\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019109 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 145638.727932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn6Zur79Jc7p",
        "outputId": "0689d820-91b4-40f0-aeaf-80310c9cd51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 22689.685743233138\n",
            "R2_score: 0.9983422495754095\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "mean_squared_error와 r2_score는 회귀 모델의 예측 성능을 평가하는 메트릭입니다.\n",
        "RMSE는 예측값과 실제값 간의 제곱 오차의 평균 제곱근을 나타내며,\n",
        "R2 스코어는 모델이 데이터의 분산을 얼마나 설명하는지를 나타냅니다.\n",
        "'''\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2score = r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "### OUTPUT ###\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2_score:\", r2score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}