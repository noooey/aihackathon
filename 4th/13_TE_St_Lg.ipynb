{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RDgwq6d2Jc7j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O6N8TpfJc7l",
        "outputId": "6e94d043-a93b-4ab5-be22-264100bc26f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84840, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
        "input_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "spuLsHiPJc7m"
      },
      "outputs": [],
      "source": [
        "df = input_data.drop(columns=['frmDist'])\n",
        "df = df.sort_values(by='date')\n",
        "\n",
        "X = df[df.drop(columns=['outtrn_cumsum','HeatingEnergyUsage_cumsum']).columns]\n",
        "Y = df[['outtrn_cumsum','HeatingEnergyUsage_cumsum']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values"
      ],
      "metadata": {
        "id": "Q0pK967nbttZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f68fT8LIJc7o"
      },
      "outputs": [],
      "source": [
        "class SimpleEnsemble(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = [MultiOutputRegressor(model) for model in models]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "model_xgb = xgb.XGBRegressor()\n",
        "GBoost = GradientBoostingRegressor()\n",
        "LightGB = lgb.LGBMRegressor()\n",
        "RF = RandomForestRegressor()\n",
        "ETR = ExtraTreesRegressor()\n",
        "DT = DecisionTreeRegressor()\n",
        "HGBR = HistGradientBoostingRegressor()\n",
        "\n",
        "models = [GBoost, LightGB, RF, model_xgb, ETR, DT, HGBR]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.base import clone\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "class StackingModel(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for model in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
        "\n",
        "        meta_train_data = np.zeros((X.shape[0], len(self.base_models) * y.shape[1]))\n",
        "\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                multi_target_instance = MultiOutputRegressor(instance)\n",
        "                self.base_models_[i].append(multi_target_instance)\n",
        "                multi_target_instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = multi_target_instance.predict(X[holdout_index])\n",
        "                meta_train_data[holdout_index, i*y.shape[1]:(i+1)*y.shape[1]] = y_pred\n",
        "\n",
        "        self.meta_model_ = MultiOutputRegressor(self.meta_model_)\n",
        "        self.meta_model_.fit(meta_train_data, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_models) * y_train.shape[1]))\n",
        "        for i, base_models in enumerate(self.base_models_):\n",
        "            for model in base_models:\n",
        "                y_pred = model.predict(X)\n",
        "                meta_features[:, i*y_train.shape[1]:(i+1)*y_train.shape[1]] += y_pred\n",
        "        meta_features /= self.n_folds\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "\n",
        "models.append(Lasso())\n",
        "model = StackingModel(base_models=models, meta_model=Ridge())\n",
        "\n",
        "model.fit(X_train.values, y_train.values)\n",
        "y_pred = model.predict(X_test.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBoqdZzyckzm",
        "outputId": "40061128-7bb3-465f-90c6-4c60fe0761c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020480 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8972\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25575.015289\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019840 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8972\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 144010.129186\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005107 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8970\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25536.284504\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004654 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8970\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 144076.992156\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005118 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8960\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25355.827780\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031711 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8960\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 143918.266927\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036720 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25532.580681\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021439 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 143363.418515\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020312 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8955\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25689.911756\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021889 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8955\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 146163.983427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn6Zur79Jc7p",
        "outputId": "3e6eafa4-4e2d-4d62-d9ab-0c3cc1a0f982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 22306.826904546775\n",
            "R2_score: 0.9983953453023178\n"
          ]
        }
      ],
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2score = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2_score:\", r2score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}