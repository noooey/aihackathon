{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDgwq6d2Jc7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ece9c528-6064-440e-d81f-444ba5110816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O6N8TpfJc7l",
        "outputId": "a7de496b-b7ab-49fc-b747-01702f6e08dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84840, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "input_data = pd.read_csv('2023_smartFarm_AI_hackathon_dataset.csv')\n",
        "input_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spuLsHiPJc7m"
      },
      "outputs": [],
      "source": [
        "df = input_data.drop(columns=['frmDist'])\n",
        "df = df.sort_values(by='date')\n",
        "\n",
        "X = df[df.drop(columns=['outtrn_cumsum','HeatingEnergyUsage_cumsum']).columns]\n",
        "Y = df[['outtrn_cumsum','HeatingEnergyUsage_cumsum']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values"
      ],
      "metadata": {
        "id": "Q0pK967nbttZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f68fT8LIJc7o"
      },
      "outputs": [],
      "source": [
        "class SimpleEnsemble(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, models):\n",
        "        self.models = [MultiOutputRegressor(model) for model in models]\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = np.array([model.predict(X) for model in self.models])\n",
        "        return np.mean(predictions, axis=0)\n",
        "\n",
        "model_xgb = xgb.XGBRegressor()\n",
        "GBoost = GradientBoostingRegressor()\n",
        "LightGB = lgb.LGBMRegressor()\n",
        "RF = RandomForestRegressor()\n",
        "ETR = ExtraTreesRegressor()\n",
        "DT = DecisionTreeRegressor()\n",
        "HGBR = HistGradientBoostingRegressor()\n",
        "\n",
        "models = [GBoost, LightGB, RF, model_xgb, ETR, DT, HGBR]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.base import clone\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "\n",
        "class StackingModel(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for model in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n",
        "\n",
        "        meta_train_data = np.zeros((X.shape[0], len(self.base_models) * y.shape[1]))\n",
        "\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                multi_target_instance = MultiOutputRegressor(instance)\n",
        "                self.base_models_[i].append(multi_target_instance)\n",
        "                multi_target_instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = multi_target_instance.predict(X[holdout_index])\n",
        "                meta_train_data[holdout_index, i*y.shape[1]:(i+1)*y.shape[1]] = y_pred\n",
        "\n",
        "        self.meta_model_ = MultiOutputRegressor(self.meta_model_)\n",
        "        self.meta_model_.fit(meta_train_data, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        meta_features = np.zeros((X.shape[0], len(self.base_models) * y_train.shape[1]))\n",
        "        for i, base_models in enumerate(self.base_models_):\n",
        "            for model in base_models:\n",
        "                y_pred = model.predict(X)\n",
        "                meta_features[:, i*y_train.shape[1]:(i+1)*y_train.shape[1]] += y_pred\n",
        "        meta_features /= self.n_folds\n",
        "        return self.meta_model_.predict(meta_features)\n",
        "\n",
        "models.append(Lasso())\n",
        "stacked_model = StackingModel(base_models=models, meta_model=Ridge())\n",
        "\n",
        "stacked_model.fit(X_train.values, y_train.values)\n",
        "y_pred = stacked_model.predict(X_test.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBoqdZzyckzm",
        "outputId": "62e8c28d-8e19-4cfa-8ccc-377d6be0ec80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004953 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8962\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25567.904864\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018732 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8962\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 145196.953353\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019142 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8969\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25559.873725\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020242 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8969\n",
            "[LightGBM] [Info] Number of data points in the train set: 54297, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 144129.245816\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25580.749741\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018948 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8961\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 143539.918085\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021548 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8956\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25681.251449\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035393 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8956\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 143303.960605\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019289 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8967\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 25299.840535\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018746 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 8967\n",
            "[LightGBM] [Info] Number of data points in the train set: 54298, number of used features: 39\n",
            "[LightGBM] [Info] Start training from score 145362.735171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn6Zur79Jc7p",
        "outputId": "80a5465c-e570-456d-f72b-86047c5e5b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 22511.365733378607\n",
            "R2_score: 0.9983668580788365\n"
          ]
        }
      ],
      "source": [
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2score = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"RMSE:\", rmse)\n",
        "print(\"R2_score:\", r2score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}